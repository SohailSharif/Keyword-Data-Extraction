{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Alethea MLE Take Home Assignment\n",
        "\n",
        "Congratulations on moving to this step of the interview process! This assignment is meant to be a practical assessment of coding that you may expect to do at Alethea. We understand that take home assignments require work beyond your other current responsibilities, and we appreciate you taking the time and effort to answer the questions here.\n",
        "\n",
        "We ask that you use Python for the assignment, as that is one of the primary programming language we use. You may use any packages that you would like to answer the questions."
      ],
      "metadata": {
        "id": "Txf7grzMwC2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "The data that you will be working with is a sample of posts about COVID-19 from December 2023. There are two datasets, `posts` and `accounts`, provided as TSVs and containing the following fields.\n"
      ],
      "metadata": {
        "id": "ohegiJQbwgKw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Posts\n",
        "|    Field   |   Type   |                               Description                              |\n",
        "|------------|----------|------------------------------------------------------------------------|\n",
        "| id         | str      | The unique identifier of the post                                      |\n",
        "| created_at | datetime | The time at which the post was made                                    |\n",
        "| author_id  | str      | The ID of the account that made the post                               |\n",
        "| is_repost  | bool     | Whether the post is a repost                                           |\n",
        "| text       | str      | The text of the post                                                   |\n",
        "| hashtags   | str      | The unique hashtags present in the post, each separated by a pipe `\\|` |"
      ],
      "metadata": {
        "id": "rgYH3kcswgO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accounts\n",
        "\n",
        "| Field       | Type     | Description                                                          |\n",
        "|-------------|----------|----------------------------------------------------------------------|\n",
        "| id          | str      | The unique identifier of the account                                 |\n",
        "| created_at  | datetime | The time at which the account was made                               |\n",
        "| screen_name | str      | The screen name of the account, also known as its handle or username |"
      ],
      "metadata": {
        "id": "9lb5QAFLwgSn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Data\n",
        "\n",
        "Please run the following `wget` commands to download the data into your environment. Then, run the Python cells to structure the data in the format that\n",
        "we will use for this assignment."
      ],
      "metadata": {
        "id": "BmYosKQWwNYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://alethea-take-home.s3.amazonaws.com/posts.tsv\n",
        "!wget https://alethea-take-home.s3.amazonaws.com/accounts.tsv"
      ],
      "metadata": {
        "id": "786C7Q53O047",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "883c73f7-83ae-4ea9-b634-94e9ccf3522f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-09 15:41:47--  https://alethea-take-home.s3.amazonaws.com/posts.tsv\n",
            "Resolving alethea-take-home.s3.amazonaws.com (alethea-take-home.s3.amazonaws.com)... 3.5.11.184, 52.217.115.193, 54.231.226.225, ...\n",
            "Connecting to alethea-take-home.s3.amazonaws.com (alethea-take-home.s3.amazonaws.com)|3.5.11.184|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 38751852 (37M) [binary/octet-stream]\n",
            "Saving to: ‘posts.tsv’\n",
            "\n",
            "posts.tsv           100%[===================>]  36.96M  66.4MB/s    in 0.6s    \n",
            "\n",
            "2024-02-09 15:41:48 (66.4 MB/s) - ‘posts.tsv’ saved [38751852/38751852]\n",
            "\n",
            "--2024-02-09 15:41:48--  https://alethea-take-home.s3.amazonaws.com/accounts.tsv\n",
            "Resolving alethea-take-home.s3.amazonaws.com (alethea-take-home.s3.amazonaws.com)... 52.217.132.73, 52.217.141.97, 52.216.216.169, ...\n",
            "Connecting to alethea-take-home.s3.amazonaws.com (alethea-take-home.s3.amazonaws.com)|52.217.132.73|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5306098 (5.1M) [binary/octet-stream]\n",
            "Saving to: ‘accounts.tsv’\n",
            "\n",
            "accounts.tsv        100%[===================>]   5.06M  21.4MB/s    in 0.2s    \n",
            "\n",
            "2024-02-09 15:41:49 (21.4 MB/s) - ‘accounts.tsv’ saved [5306098/5306098]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl"
      ],
      "metadata": {
        "id": "LH7a2a--wEte"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_posts = (pl\n",
        "    .read_csv(\n",
        "        'posts.tsv',\n",
        "        separator='\\t',\n",
        "        try_parse_dates=True\n",
        "    ).with_columns(\n",
        "        hashtags=pl.col('hashtags').str.split('|')\n",
        "    )\n",
        ")\n",
        "\n",
        "df_accts = pl.read_csv(\n",
        "    'accounts.tsv',\n",
        "    separator='\\t'\n",
        ")"
      ],
      "metadata": {
        "id": "iq5KXFD-blof"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posts = df_posts.to_dicts()\n",
        "accts = df_accts.to_dicts()"
      ],
      "metadata": {
        "id": "7RU6fLJ5Ep7T"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posts"
      ],
      "metadata": {
        "id": "yJEBIj1DkE3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questions"
      ],
      "metadata": {
        "id": "cWxF8d_6wbNJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1\n",
        "\n",
        "Given a set of posts, write a function that returns all of the accounts that authored posts using a certain hashtag, _not_ including reposts. The function should follow the provided model signature below.\n",
        "\n",
        "Use the function to determine the number of accounts that posted the hashtag #diedsuddenly."
      ],
      "metadata": {
        "id": "rH7UYsXKwSbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List, Tuple\n",
        "\n",
        "def get_hashtag_accounts(\n",
        "        posts: List[Dict],\n",
        "        hashtag: str\n",
        "    ) -> List[str]:\n",
        "    \"\"\"\n",
        "    From a set of posts, returns the IDs of all accounts that posted a\n",
        "    particular hashtag (excluding reposts)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    posts:\n",
        "        A list of dictionary records, where each record corresponds to one post.\n",
        "        Keys are metadata fields for a post, and values are the metadata itself\n",
        "    hashtag:\n",
        "        The hashtag by which to filter\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    accounts:\n",
        "        The IDs of all accounts that posted the `hashtag` (excluding reposts)\n",
        "    \"\"\"\n",
        "    accounts = set()\n",
        "    for post in posts:\n",
        "        if post['hashtags'] is not None and hashtag in post['hashtags'] and not post['is_repost']:\n",
        "            accounts.add(post['author_id'])\n",
        "    return list(accounts)"
      ],
      "metadata": {
        "id": "yhpu0jgVwZ-4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filt_hashtag = '#diedsuddenly'\n",
        "#filt_hashtag = '#Pfizer'\n",
        "filt_accts = get_hashtag_accounts(posts=posts, hashtag=filt_hashtag)\n",
        "\n",
        "print(f\"{len(filt_accts):,} accounts posted the hashtag {filt_hashtag}\")"
      ],
      "metadata": {
        "id": "1xp4MRJAEe3C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edec1bd4-c322-4f53-cf8a-06490ca74bfc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33 accounts posted the hashtag #diedsuddenly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2\n",
        "\n",
        "Accounts that are all controlled by a single individual or organization using automation or semi-automation are often referred to as _sock puppets_. An indicator that a group of accounts may be sock puppets is that they all have similar screen names. For example, a disinformation campaign could pose as fake New York City news outlets with the screen names \"breaking_news_nyc\", \"breaking_news_ny\", and \"breaking_news_nyc2\".\n",
        "\n",
        "One way to measure the similarity of screen names is by using the [Damerau-Levenshtein distance](https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance), an edit distance between two strings. It can be normalized by the maximum possible distance to produce the normalized Damerau-Levenshtein similarity, which is 1 if two strings are exactly the same and 0 if they are maximally different. A [performant implementation](https://rapidfuzz.github.io/RapidFuzz/Usage/distance/DamerauLevenshtein.html#normalized-similarity) of the similarity measure is available in the package RapidFuzz.\n",
        "\n",
        "Given a set of accounts, write a function that identifies all _unique pairs_ of accounts that have similar screen names. The function should follow the provided model signature below.\n",
        "\n",
        "Use the function to determine the number of _unique pairs_ of accounts that posted the hashtag #diedsuddenly (not including reposts), and that have screen names with a similarity greater than 0.8."
      ],
      "metadata": {
        "id": "SLM29VCkw5Ny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rapidfuzz\n",
        "from rapidfuzz.distance.DamerauLevenshtein import normalized_similarity"
      ],
      "metadata": {
        "id": "aDD2LOzbw5XX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5292e29-f067-4afa-f6ac-86eebdd4a11c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rapidfuzz\n",
            "  Downloading rapidfuzz-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz\n",
            "Successfully installed rapidfuzz-3.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similar_screen_names(\n",
        "      accounts: List[Dict],\n",
        "      min_similarity: float\n",
        "    ) -> List[Tuple[str, str, float]]:\n",
        "    \"\"\"\n",
        "    From a set of accounts, returns pairs of accounts that have similar screen\n",
        "    names according to the normalized Damerau-Levenshtein similarity\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    accounts:\n",
        "        A list of account records, where each record corresponds to one account.\n",
        "        Keys are metadata fields for an account, and values are the metadata\n",
        "        itself\n",
        "    min_similarity:\n",
        "        A value between 0 and 1, indicating the minimum similarity needed to\n",
        "        determine two accounts have similar screen names\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    similar_account_pairs:\n",
        "        A list of tuples of the format (account_id1, account_id2) indicating\n",
        "        which accounts have similar screen names\n",
        "    \"\"\"\n",
        "    pass"
      ],
      "metadata": {
        "id": "pW3iA3KqxfKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "def get_similar_screen_names(accounts, min_similarity):\n",
        "    similar_account_pairs = []\n",
        "\n",
        "    for acc1, acc2 in combinations(accounts, 2):\n",
        "        screen_name1 = acc1.get('screen_name', '')\n",
        "        screen_name2 = acc2.get('screen_name', '')\n",
        "\n",
        "        similarity = normalized_similarity(screen_name1, screen_name2) / 100.0\n",
        "\n",
        "        if similarity >= min_similarity:\n",
        "            similar_account_pairs.append((acc1['account_id'], acc2['account_id'], similarity))\n",
        "\n",
        "    return similar_account_pairs\n"
      ],
      "metadata": {
        "id": "it-PiiqXqJ_4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hashtag_accts = [acct for acct in accts if acct['id'] in filt_accts]\n",
        "sim_accts = get_similar_screen_names(accounts=hashtag_accts, min_similarity=0.8)\n",
        "\n",
        "print(f\"{len(sim_accts):,} pairs of accounts that posted the hashtag {filt_hashtag} have similar screen names\")"
      ],
      "metadata": {
        "id": "pqG8EW9iHyqw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbf63145-3d32-417c-b0ba-f499195dd555"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 pairs of accounts that posted the hashtag #diedsuddenly have similar screen names\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3\n"
      ],
      "metadata": {
        "id": "CSMNL_aXB5hM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have a simple \"model\" to identify all pairs of accounts that posted a particular hashtag and have similar screen names. Suppose that we want to be able to access it via an API. Specifically, we want to create an API endpoint to which we can make GET requests, where it accepts the following request parameters and yields the following respose:\n",
        "\n",
        "**Input**\n",
        "- `posts`: A list of post records, where each record corresponds to one post\n",
        "- `accounts`: A list of account records, where each record corresponds to one account\n",
        "- `hashtag`: The hashtag by which to filter\n",
        "- `min_similarity`: The minimum similarity to determine two screen names are similar\n",
        "\n",
        "**Output**\n",
        "- `similar_account_pairs`: A list of tuples of the format (`account_id1`, `account_id2`) indicating which accounts have similar screen names\n",
        "\n",
        "Please create an endpoint according to the specified parameters using Flask or FastAPI. All input and output should be formatted using JSON.\n",
        "\n",
        "Feel free to do this in a different environment, and submit a link to a Github repo with deployment instructions."
      ],
      "metadata": {
        "id": "OKCTWPS7w6FP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g88Iwo6Tdgdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4"
      ],
      "metadata": {
        "id": "2YUm9sbxG-SD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given an API endpoint for our model, we want to be able to serve it in an arbitrary computing environment. Write a `requirements.txt` file and Dockerfile that could be used to containerize the model and access it via its API. You do _not_ have to build and run the container in this notebook."
      ],
      "metadata": {
        "id": "704LJCDEHFQG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SCf-9hR5ZLZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5"
      ],
      "metadata": {
        "id": "lnj4g_KRH5zV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose that we have found our model performs well in practice, and we now want to deploy it to Artemis, our SaaS offering for detecting, assessing, and mitigating disinformation and other online harms. In production, there is a significantly higher volume of data, both in terms of the number of posts and accounts, and the number of users who are making concurrent requests to the model.\n",
        "\n",
        "How would you ensure and evaluate that the model is performant in these circumstances? What monitoring may you want to put in place?"
      ],
      "metadata": {
        "id": "GrYFpz7RH9D4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ],
      "metadata": {
        "id": "xatyGA-AN5gH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VImxY557N7Dz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}